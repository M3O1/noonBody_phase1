{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from model import get_1000fps_model, get_basic_unet_model\n",
    "from data import dataGenerator, load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras import Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Conv2D, MaxPool2D, ZeroPadding2D, Conv2DTranspose, MaxPooling2D\n",
    "from keras.layers import BatchNormalization, Flatten, Dense, Reshape, concatenate\n",
    "\n",
    "from keras.models import model_from_json\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IoU metric\n",
    "def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis=0)\n",
    "\n",
    "def extract_contour_mask(mask):\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    erosion = cv2.erode(mask, kernel, iterations =1)\n",
    "    return mask - erosion\n",
    "\n",
    "def save_test_images(pred,images):\n",
    "    pred = (pred < 0.5).astype(np.uint8)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(pred.shape[0]):\n",
    "        mask = extract_contour_mask(pred[i])\n",
    "        mask = np.stack([mask*255,mask*0,mask*0],axis=-1)\n",
    "        mask = cv2.resize(mask,(256,256))\n",
    "        results.append(cv2.add(images[i],mask))\n",
    "    results = np.concatenate(results)\n",
    "    \n",
    "    results = cv2.cvtColor(results,cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(os.path.join(model_dir,\"example_output.png\"),results)\n",
    "    \n",
    "def load_example_images(example_dir):\n",
    "    images = []\n",
    "    inputs = []\n",
    "    for filename in os.listdir(example_dir):\n",
    "        ex_path = os.path.join(example_dir,filename)\n",
    "        image = cv2.imread(ex_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        input_ = cv2.resize(image,input_size)\n",
    "        input_ = input_/255.\n",
    "        image = cv2.resize(image,(256,256))\n",
    "        inputs.append(input_)\n",
    "        images.append(image)\n",
    "\n",
    "    inputs = np.stack(inputs,axis=0)\n",
    "    return inputs, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up\n",
    "\n",
    "## model setting\n",
    "filters = 32\n",
    "depth = 3\n",
    "input_size = (192,192)\n",
    "batch_size = 64\n",
    "\n",
    "model = get_basic_unet_model(input_size, depth=depth, filters=filters)\n",
    "model.compile(\"adam\",\n",
    "              loss=\"mse\",\n",
    "             metrics=['mse',mean_iou])\n",
    "\n",
    "## test example sample\n",
    "example_dir = \"./data/example/\"\n",
    "inputs, images = load_example_images(example_dir)\n",
    "\n",
    "## load data generator\n",
    "dataset = load_dataset(input_size=input_size)\n",
    "trainset, testset = train_test_split(dataset,test_size=0.1)\n",
    "\n",
    "train_nums = len(trainset)\n",
    "train_steps = train_nums // batch_size\n",
    "\n",
    "test_nums = len(testset)\n",
    "test_steps = test_nums // batch_size\n",
    "\n",
    "traingen = dataGenerator(trainset, input_size, batch_size)\n",
    "testgen = dataGenerator(testset, input_size, batch_size)\n",
    "\n",
    "tqdm = TQDMNotebookCallback()\n",
    "model_dir = os.path.join(\"../save/\",datetime.now().strftime(\"unet_{}_{}-%m%d-%H%M\").format(depth,filters))\n",
    "os.makedirs(model_dir,exist_ok=True)\n",
    "modelcheckpoint = ModelCheckpoint(os.path.join(model_dir,\"model_{val_loss:.4f}.h5\"),\n",
    "                                  monitor='val_loss',\n",
    "                                 save_best_only=True)\n",
    "earlystopping = EarlyStopping(monitor='val_loss',patience=10)\n",
    "\n",
    "callbacks= [tqdm, modelcheckpoint, earlystopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = model.fit_generator(traingen,\n",
    "                           train_steps,\n",
    "                           epochs=100,\n",
    "                           verbose=0, \n",
    "                           validation_data = testgen,\n",
    "                           validation_steps = test_steps,\n",
    "                           callbacks=callbacks)\n",
    "\n",
    "pred = model.predict_on_batch(inputs)\n",
    "save_test_images(pred,images)\n",
    "\n",
    "# Draw the results\n",
    "train_mse = hist.history['mean_squared_error']\n",
    "valid_mse = hist.history['val_mean_squared_error']\n",
    "train_iou = hist.history['mean_iou']\n",
    "valid_iou = hist.history['val_mean_iou']\n",
    "df = pd.DataFrame({\"mse-train\":train_mse,\n",
    "                   \"mse-valid\":valid_mse,\n",
    "                   \"iou-train\":train_iou,\n",
    "                   \"iou-valid\":valid_iou})\n",
    "df.plot().figure.savefig(os.path.join(model_dir,\"training.png\"))\n",
    "    \n",
    "# Save Model\n",
    "## serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(model_dir,\"model.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "## serialize weights to HDF5\n",
    "model.save_weights(os.path.join(model_dir,\"model.h5\"))\n",
    "\n",
    "## Open the file\n",
    "with open(os.path.join(model_dir,'report.txt'),'w') as fh:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

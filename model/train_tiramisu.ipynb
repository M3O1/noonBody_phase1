{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #2\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/matplotlib/__init__.py:1067: UserWarning: Duplicate key in file \"/home/ubuntu/.config/matplotlib/matplotlibrc\", line #3\n",
      "  (fname, cnt))\n",
      "/home/ubuntu/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "from model import get_1000fps_model, get_basic_unet_model\n",
    "from data import dataGenerator, load_dataset\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras import Model\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Conv2D, MaxPool2D, ZeroPadding2D, Conv2DTranspose, MaxPooling2D\n",
    "from keras.layers import Flatten, Dense, Reshape, concatenate\n",
    "from keras.layers.core import Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.models import model_from_json\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define IoU metric\n",
    "def mean_iou(y_true, y_pred):\n",
    "    prec = []\n",
    "    for t in np.arange(0.5, 1.0, 0.05):\n",
    "        y_pred_ = tf.to_int32(y_pred > t)\n",
    "        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)\n",
    "        K.get_session().run(tf.local_variables_initializer())\n",
    "        with tf.control_dependencies([up_opt]):\n",
    "            score = tf.identity(score)\n",
    "        prec.append(score)\n",
    "    return K.mean(K.stack(prec), axis=0)\n",
    "\n",
    "def extract_contour_mask(mask):\n",
    "    kernel = np.ones((3,3), np.uint8)\n",
    "    erosion = cv2.erode(mask, kernel, iterations =1)\n",
    "    return mask - erosion\n",
    "\n",
    "def save_test_images(pred,images):\n",
    "    pred = (pred < 0.5).astype(np.uint8)\n",
    "    \n",
    "    results = []\n",
    "    for i in range(pred.shape[0]):\n",
    "        mask = extract_contour_mask(pred[i])\n",
    "        mask = np.stack([mask*255,mask*0,mask*0],axis=-1)\n",
    "        mask = cv2.resize(mask,(256,256))\n",
    "        results.append(cv2.add(images[i],mask))\n",
    "    results = np.concatenate(results)\n",
    "    \n",
    "    results = cv2.cvtColor(results,cv2.COLOR_RGB2BGR)\n",
    "    cv2.imwrite(os.path.join(model_dir,\"example_output.png\"),results)\n",
    "    \n",
    "def load_example_images(example_dir):\n",
    "    images = []\n",
    "    inputs = []\n",
    "    for filename in os.listdir(example_dir):\n",
    "        ex_path = os.path.join(example_dir,filename)\n",
    "        image = cv2.imread(ex_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        input_ = cv2.resize(image,input_size)\n",
    "        input_ = input_/255.\n",
    "        image = cv2.resize(image,(256,256))\n",
    "        inputs.append(input_)\n",
    "        images.append(image)\n",
    "\n",
    "    inputs = np.stack(inputs,axis=0)\n",
    "    return inputs, images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denseBlock(inode, nb_layers, growth_rate, dropout_rate=0.2):\n",
    "    for _ in range(nb_layers):\n",
    "        tmp = inode\n",
    "        inode = BatchNormalization(gamma_regularizer=l2(0.0001),\n",
    "                              beta_regularizer=l2(0.0001))(inode)\n",
    "        inode = Activation('relu')(inode)\n",
    "        inode = ZeroPadding2D((1,1))(inode)\n",
    "        inode = Conv2D(growth_rate, \n",
    "                        kernel_size=(3, 3), \n",
    "                        kernel_initializer='he_uniform')(inode)\n",
    "        inode = Dropout(dropout_rate)(inode)\n",
    "        inode = concatenate([inode, tmp])\n",
    "    return inode\n",
    "\n",
    "def transitionDown(inode, nb_features,dropout_rate=0.2):\n",
    "    inode = BatchNormalization(gamma_regularizer=l2(0.0001),\n",
    "                            beta_regularizer=l2(0.0001))(inode)\n",
    "    inode = Activation('relu')(inode)\n",
    "    inode = Conv2D(nb_features, \n",
    "                   kernel_size=(1, 1),\n",
    "                   kernel_initializer='he_uniform')(inode)\n",
    "    inode = Dropout(dropout_rate)(inode)\n",
    "    inode = MaxPooling2D(pool_size=(2, 2), strides=2)(inode)\n",
    "    return inode\n",
    "\n",
    "def get_tiramisu_model(input_size, layer_per_block, growth_rate, nb_features=32):\n",
    "    input_shape = (*input_size, 3)\n",
    "    n_pool = len(layer_per_block) - 1\n",
    "\n",
    "    x = Input(input_shape)\n",
    "\n",
    "    inode = ZeroPadding2D((1,1))(x)\n",
    "    inode = Conv2D(nb_features,(3,3))(inode)\n",
    "\n",
    "    # dense block\n",
    "    skip_connections = []\n",
    "    for i in range(n_pool):\n",
    "        inode = denseBlock(inode, layer_per_block[i],growth_rate)\n",
    "        skip_connections.append(inode)\n",
    "        nb_features += growth_rate * layer_per_block[i]\n",
    "        inode = transitionDown(inode, nb_features)\n",
    "\n",
    "    inode = denseBlock(inode, layer_per_block[n_pool],growth_rate) # bottle neck\n",
    "    skip_connections = skip_connections[::-1]\n",
    "    layer_per_block = layer_per_block[::-1]\n",
    "\n",
    "    for i in range(n_pool):\n",
    "        keep_nb_features = growth_rate * layer_per_block[i+1]\n",
    "        inode = Conv2DTranspose(keep_nb_features,\n",
    "                                strides=2,\n",
    "                                kernel_size=(3, 3),\n",
    "                                padding=\"same\",\n",
    "                                kernel_initializer='he_uniform')(inode)\n",
    "        inode = concatenate([inode, skip_connections[i]])\n",
    "\n",
    "        inode = denseBlock(inode, layer_per_block[i+1],growth_rate)\n",
    "\n",
    "    inode = Conv2D(1, kernel_size=(1, 1), kernel_initializer='he_uniform',activation='sigmoid')(inode)\n",
    "    y = Reshape(input_size)(inode)\n",
    "    model = Model(inputs=x, outputs=y)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set-up\n",
    "\n",
    "## model setting\n",
    "layer_per_block = [5,6,7] \n",
    "growth_rate = 12 \n",
    "nb_features = 32\n",
    "input_size = (192,192)\n",
    "batch_size = 4\n",
    "\n",
    "model = get_tiramisu_model(input_size, layer_per_block, growth_rate, nb_features)\n",
    "model.compile(\"adam\",\n",
    "              loss=\"mse\",\n",
    "             metrics=['mse',mean_iou])\n",
    "\n",
    "## test example sample\n",
    "example_dir = \"./data/example/\"\n",
    "inputs, images = load_example_images(example_dir)\n",
    "\n",
    "## load data generator\n",
    "dataset = load_dataset(input_size=input_size)\n",
    "trainset, testset = train_test_split(dataset,test_size=0.1)\n",
    "\n",
    "train_nums = len(trainset)\n",
    "train_steps = train_nums // batch_size\n",
    "\n",
    "test_nums = len(testset)\n",
    "test_steps = test_nums // batch_size\n",
    "\n",
    "traingen = dataGenerator(trainset, input_size, batch_size)\n",
    "testgen = dataGenerator(testset, input_size, batch_size)\n",
    "\n",
    "tqdm = TQDMNotebookCallback()\n",
    "model_dir = os.path.join(\"../save/\",datetime.now().strftime(\"tiramisu_{}-%m%d-%H%M\").format(growth_rate))\n",
    "os.makedirs(model_dir,exist_ok=True)\n",
    "modelcheckpoint = ModelCheckpoint(os.path.join(model_dir,\"model_best.h5\"),\n",
    "                                  monitor='val_loss', save_best_only=True,save_weights_only=True)\n",
    "earlystopping = EarlyStopping(monitor='val_loss',patience=10)\n",
    "\n",
    "callbacks= [tqdm, modelcheckpoint, earlystopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b36a5b16f5124523bccfde614f28fbd9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Training'), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92fb4d54e0a840338a4d53f846468fd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch 0', max=1212), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(traingen,\n",
    "                           train_steps,\n",
    "                           epochs=100,\n",
    "                           verbose=0, \n",
    "                           validation_data = testgen,\n",
    "                           validation_steps = test_steps,\n",
    "                           callbacks=callbacks)\n",
    "\n",
    "pred = model.predict_on_batch(inputs)\n",
    "save_test_images(pred,images)\n",
    "\n",
    "# Draw the results\n",
    "train_mse = hist.history['mean_squared_error']\n",
    "valid_mse = hist.history['val_mean_squared_error']\n",
    "train_iou = hist.history['mean_iou']\n",
    "valid_iou = hist.history['val_mean_iou']\n",
    "df = pd.DataFrame({\"mse-train\":train_mse,\n",
    "                   \"mse-valid\":valid_mse,\n",
    "                   \"iou-train\":train_iou,\n",
    "                   \"iou-valid\":valid_iou})\n",
    "df.plot().figure.savefig(os.path.join(model_dir,\"training.png\"))\n",
    "    \n",
    "# Save Model\n",
    "## serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(os.path.join(model_dir,\"model.json\"), \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "## serialize weights to HDF5\n",
    "model.save_weights(os.path.join(model_dir,\"model.h5\"))\n",
    "\n",
    "## Open the file\n",
    "with open(os.path.join(model_dir,'report.txt'),'w') as fh:\n",
    "    # Pass the file handle in as a lambda function to make it callable\n",
    "    model.summary(print_fn=lambda x: fh.write(x + '\\n'))\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow_p36)",
   "language": "python",
   "name": "conda_tensorflow_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
